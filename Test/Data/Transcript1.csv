article_id,article_text
1,"Hi, and welcome to our second to the last lecture. This lecture is on Poisson GLNs, and I should give some credit to Jeff Leek who I got much of this content from, from an earlier version of this class. So modeling count data arises quite frequently in applications. For example, the number of calls to a call center, the number of flu cases. And in each of these cases, the counts are unbounded in the sense of well, there might be some theoretical bounded account, the total number of people in the world or whatever. However, we don't really know what that is or that number is really large, relative to the count that we're looking at. So in addition to count, data can come in the form of rates or proportions, such as the percentage of people passing a test, or in terms of rates, think about the number of cases or something like that, that occur over a unit of time. My favorite example is from a nuclear pump failure experiment where we're looking at the number of instances that nuclear pumps failure, per, failed per unit time. So that would be a rate. A very common rate that occurs in bio statistics and public health. Where I work in is, the so called incidence rate, which is the number of newly developed cases per person time at risk. Okay so all of these are instances of counts and rates and proportions are also you can think of as counts. Both because the numerator is a count and whatever you're dividing by either the percent time at risk or the total time or the total sample or the number of trials or something like that. That's a second number that we're going to show you how to deal with as well when looking at the numerator, the count part, okay? And all of these can be handled with Poisson GLMs. So the Poisson Distribution is a useful model for counts and rates. And again a rate is a count per some monitoring time. So, often incidence rates and web traffic and all these other things are modeled by Poisson distributions. A very common use of the Poisson distribution is approximating binomial probabilities where the success probability is very small and the end is very large, so you can think of that as an instance of the sort of approximated and unbounded count, even though the actual count is bounded. And then an application that I like quite a bit is so called Contingency Table data. So if you have an instance where you just counted the number of occurrences of a different collection of variables. So if took a random sample of people and I counted the number of people that had blonde hair, brown hair and black hair and I cross tabulated that with the number of people who had blue eyes, brown eyes and hazel eyes. Okay that table of counts is called a contingency table and Poisson models are very useful for modeling contingency table data. They give a very elegant framework for doing that. I give the Poisson mass function here and so the rate of counts per unit time is lambda, whereas t is the total time. If x is a plus on with this mean, then its expected value is t times lambda. So the expected value is plus sign Is that's is the t times lambda. So our natural estimate of the rate would be the count over the total time okay? So x over t and it's nice to know in this case that the expected value of x over t the expected value of our rate estimate is exactly lambda. The rate that we would like the estimate. So, that's the useful property associated with the Poisson. The variance is equal to the mean, so the variance is e lambda. So that's the assumption of our model that we can check and we have some potential solutions of it's doesn't hold. And another interesting fact is the Poisson tends to a normal as the mean gets large. So you can think of this in several ways. All that has to happen is for t lambda to get large. This could occur if t is fixed and lambda gets large, if lambda is fixed and t gets large, or both of them get large. And in a lot of different applications the way in which the mean gets large could vary but as long as it gets large in some sense then the Poisson is going to approximate a normal distribution. And here I show you this via simulation. I simulate three different collections of Poisson random variables as the mean of the Poisson distribution gets larger and larger and you can see by the right most panel that it's nearly identical to a normal distribution at that point. And then, we can actually show that we don't, if this isn't the appropriate class the fact to show the mathematics that the meaning of variants are equal theoretically so, a way could do that by simulation and I do that here where I right, are not, I'm sorry this is an access simulation. We're actually try to show it using the density and summing up the density in the right way. So if you're interested try that experiment and it will prove to you that the meaning of variance or equal, try it for bunch of different scenarios. Or you could just believe me or you could take for example mathematical biostatistics boot camp one or two are my other course or classes. Where we cover how to do the actual mathematics for this. So as an example, let's look at Jeff Leek, his web traffic. So this is his website, www.biostat, or I'm sorry, biostat.jhsph.edu/~jleek. And the place I mean in this case is the interpretive a number of web hits per day. So our unit our time in this case is T equal to one. Now for the one to interpret the length that we estimate as web hits per hour we would have to put the T equal 24. So I hope you understand that and if you want it to have it to be seconds you need to put a T equal 24 times or minutes it would have to be T equal 24 times 60 and so on. Let's look at the data I show here how you can download it and I convert the date from a standard character date time format to a Julian date. Julian date counts the number of days since January 1st 1970 I believe. So the Julian date is nice to think about because it's just a count. It's the number of days whereas the date is kind of a complicated format because it's characters. So when you do the head of the date here, you see the date which is in character format. You see the number of visits, and he is not doing so well. These early dates with 0 visits on all those dates. The number of visits that originate from simply statistics and the julian date. So here's a plot of the data set. The Julian date is on the x axis and the number of visits is on the y axis. Now, we've covered in the last lecture what linear regression, some of the shortfalls of linear regression is try to model count data or in that case, binary data. So let's not just re-hatch that same topic, there are some issues with modelling count data as if it was with a linear model directly. However, as we saw a couple of slides ago, as the mean of the counts gets larger and larger are concerned over this decreases quite a bit simply because it's going to trend to a normal distribution. So, if you have extremely large counts, this becomes a lot less objectionable. So, that's just for notation number of heads, NH is going to be our outcome JD, is the Julian day, that's going to be our predictor and this would be a linear regression model, we can plot it and see the fitted line that we would get. It has some issues. Clearly there's some curvature there, maybe we should have put an x squared term in. But that would be our first approach to this, and honestly it wouldn't be that bad. But the counts are kind of small, so it's not the best thing in the world. The interpretation isn't great for linear models, then we'll see some ways which in the next couple of slides, how we can tweak linear models to maybe get a slightly better interpretation. I think that of counts in web hits and things like that as things that you would want to think about on a relative scale and the linear model really treats it on a linear additive scale. So let's think about how we could get relative interpretations from our linear model. The first thing we might try is taking the log of the outcome, here I knew the natural log. And this would be our model, log of NH is the linear regression model. B0 + B1(JDi) + ei. Now let me speak a little bit about log and what it's accomplishing. The quantity e to the expected value of the log of a random variable is what I would call the population geometric mean. And the reason I would call it the population geometric mean is the empirical or just geometric mean is the product of a sample, product Yi, raised to the one over n power. So this the way to think about this, the product of yi to the one over n power. If we take a log of that, we get the arithmatic mean, the ordinary mean of the log data. So the geometric mean is just exponentiating the arithmatic mean of the log data. And we know that if we collect a lot of data, a lot more data in our sample, the arithmetic mean will converge to something. So the geometric mean is what this quantity, the product of the data, rays to the one over nth power, what it converges to. So, what, it turns out, when you take the log of the natural log of the outcome in a linear regression then, your exponentiated coefficients are interpretable with respect to geometric means. So, for example, E to the Beta of zero is the estimated geometric mean hits on day zero and I should reiterate the point from earlier on in the class. This intercept doesn't mean that much because January first 1970 is not a date that we care about in terms of number of web hits. So probably to make the intercept more interpretable, what we should have done is subtracted off the earliest date that we saw and started counting days from there. From all of the remaining days in our data set and then the intercept would be the e to the inner estimated intercept would be the geometric mean hits on the first day of this data set. Okay. So that's a small point but it doesn't change the fitted model. It doesn't change the slope or anything like that to shift around the intercept however nonetheless, if you want an interpretable intercept as we know from earlier on in the class, you have to do something like that. E to the beta1 on the other hand is the estimated relative increase or decrease in the geometric mean hits per day, okay? So the increase per day. So I should also mention there's a problem with logs. If you have zero counts you have to do something because you can't take the log of zero, so you need to add a constant. A very common constant that is plus one. So we do log of the outcome plus one. So if we do that, here I fit the linear model to the log of the outcome plus one versus the Julian date. We get the intercept which is kind of irrelevant in this case as we talked about before. And then we get 1.002. This is on the exponentiated scale. Okay so what that means is our model is estimating a 0.2% increase in web traffic per day. Okay? And that's a nice interpretation. If you added other covariates then that would be 0.02% increase per day holding the other covariates fixed."
2,"So let's start about Linear vs Poisson regression. So remember in GLMs, we don't love the outcome itself, or we don't take a transformation of the outcome itself, we take transformation of the mean of the outcome. So linear models, our outcome is the linear component plus the error, or we could just write that as the expected value of the outcome is the linear compnent. In a Poisson log-linear model, it's the log of the expected outcome that is the linear part. Log of the expected number of web hits per day is b0 + b1 times the Julian date. We could reverse that process by exponentiating both sides of that equation and just say the mean web hits per day now, depends on E to the linear regression model, okay? So, that's the main difference, is that we're going to assume our data is Poisson distributed with a mean. And that mean takes this form, E to the b0 + b1 times the regressor. Okay, and that's the main difference, though it changes the interpretation a lot. We get a distribution that's much more believable for our observed outcomes, okay. And we get relative interpretations because everything's logged. Our coefficients are going to be interpreted in a relative sense, just like when we logged the outcome. Though we're going to avoid problems like taking logs of 0, like we had on the previous slide, okay. Now, I want to reiterate, taking logs of your outcomes is actually, often, a very good thing to do. That's a trick that you should apply, not just for count data, but in general on regression. If you have positive data, log is one of the best transformations you can do, it's extremely helpful. The coefficients remain as, if not more, interpretable on the log scale. So that's a great transformation to do. Some of the other transformations, could be square root or cube root data, then it gets hard. Okay, so let's just remind ourselves that the differences that we're looking at are now multiplicative when we transform back to the natural scale. So if we've look at our model, it's the expected value of the outcome is E to the beta naught plus beta one times the Julian date. Well, by the properties of expected value, we can factor out beta naught in beta one times the Julian date. Now, if we looked at what would be the expected mean for the next day, the Julian date plus one, right, that would be e to the b0 + b1 (JD + 1), okay? So divide this by that, and you get e to the B1. So our coefficient E to our slope coefficient is interpreted as the relative increase or decrease in the mean per one unit change in the regressor, okay? And so if we exponentiate our coefficient, we're going to be looking at whether or not they're close to 1. If we leave them on the log scale, we're going to be looking at whether or not they're close to 0. And, again, all of these interpretations, when we extend them to the mutivariant setting, E to the beta one is the expected relative increase or decrease in web traffic, holding the other coefficient, holding the other regressors constant. Okay? So I'm hoping at this point that most of this stuff is kind of old hat for you. Okay, so here is our fitted Poisson regression model overlayed onto our data. And you can see it actually fits pretty closely to the linear model, though it has some nice curvature to it, which is what we wanted. We could have accomplished that in our linear model by adding a squared term, of course, but it's nice to note that a simpler model with fewer coefficients seems to fit the data better. A concern, often, is that the variance has to equal the mean. So the variance, in this case, needs to go up as the mean goes up. But here if we plot the fitted values versus the residuals, it's very clear that the variance is higher for lower mean values. That's the problem, okay. So we need at least some way to account for the fact that the variance is not necessarily constant. There's a lot of ways to do that, and if you read in the book, one thing that we talk about are the quasi-Poisson models. This model would look at the variance being a constant multiple of the mean, rather than being equal to the mean. But, in this case, that doesn't appear to be the case in this case because it looks like we have this issue where there's larger variance for lower fitted values, when the Poisson model assumes the opposite. So Jeff actually had this code from this model, the sandwich, which seems like a funny name for a package. But it comes from the sandwich variance estimator, made famous by generalized estimating equations, which by the way was a technique that was invented here at Johns Hopkins Biostatistics by two very well-known professors here, Scott Zeger and Kung-Yee Liang. At any rate, Jeff did some code here for getting model agnostic standard errors. And if you read in the book, there's a little bit more discussion about this. This is kind of a more advanced topic than we would like to delve into in this class. However, it's a very important applied topic, as well. It's not just a theoretical exercise. So, the main point is to do some residual plots, to understand whether or not you think your model's assumptions hold. Try quasi-Poisson model because that's a very easy thing to do in R, if you think at least it holds at some level, but maybe not just in the sense of the variance being a constant multiple of the mean. But if it really fails, like in this case, then you have to dig in to some other solutions. So, in this case, you see here's the confidence interval on the top if we don't do anything. If we do the model agnostic confidence interval, you get on the bottom. In this case, it doesn't actually make that big of a difference between the two. And, again, these are both, of course, non-exponentiated. If you want to exponentiate them, which by the way, I should also mention, exponentiating for a small coefficient like this it basically just adds 1. So, probably, if I were to enter this into R, this would be about 1.002. Just like before, about a 2% increase on the low end being estimated, 0.21% increase estimated on the low end. Maybe, at the highest possible, 0.3% increase per day on the high end. So how do you handle rates? So I should say rates and proportions because I like to distinguish between rates and proportions. So this is an instance where you have a count, and then you have some offset that should tell you how large or small the count should be. So, for example, if I'm counting failures of my nuclear pumps that I mentioned before, I should have more failures if I monitored them for a longer time. If I'm counting the number of flu cases, I should have more flu cases if I'm looking at a larger population, right. So I should have more flu cases in a bigger city than I would have in a smaller city. So in all these cases, if the counts that we're interested in have some term that we really want to interpret our count relative to that, whether it's a unit of time, person, time at risk, total sample size, then what we want to do, and it's quite simple how to do this in R. The first thing we note is that we want to actually interpret not the expected value of the outcome, but the expected value of the outcome divided by this relative term, whether it's monitoring time, person, time at risk or whatever. So, in this case, Jeff is looking at the number of web hits originating from Simply Statistic, relative to the total number of web hits, okay? And he wants to model that as E to the b0 + b1 times the Julian date, so he want to model that proportion as being this log-linear model. Well, if you take the log of both sides, and we mess around a little bit, we see that we get kind of a similar model to what we had before. We get the log the outcome is the linear regression part, but that also it has this log offset with no coefficient. And that, it turns out, is all you have to do to add a regular proportion into a Poisson GLM. You just take whatever this relative denominator count or time or whatever it is that you want to consider, and add it as a log offset in your linear model. Okay, so the easy way to add this offset into our linear model is just to use of the term offset equals log visits plus 1. Remember, we have to add the plus 1 because we can't take the log of 0. Remember, we have to have family equals Poisson in the model statement here, and by default it assumes a log link which is what we want, we haven't really covered any other kinds. So, there's another way you can add the offset. I think you can add a plus 0 variable name in the actual model specification part on the right of the tilde, but this is just as easy to do it as an argument in the GLM function. Here, Jeff gives the, the difference between the GLM1 fitted rates, which was, remember, to the number of web hits, versus the GLM2 fitted rates, the assigned variable GLM2 fitted rates, which was the relative number of web hits originating from Simply Statistics. So, these blue points are adjusted for the red points in a sense. And this actually shows the fitted model relative to the data, there are a lot of zeros early on, and then it took off quite a bit. But this is the fitted model, the blue line is the fitted model, and the gray points are the actual data points. So you can get more information on log-linear models and multiway tables. And another very common problem that occurs in Poisson data is when the particular number, zero, occurs way more often that it should. This is called zero inflation. Then the Poisson model would allow it to, if it wants to fit the other counts. And so this is called zero inflation. And so there's a lot of different ways to handle zero inflation in Poisson data, but you need to think about that. In this case, yeah, we might be concerned with handling all of those zeros early on, though there's a temporal component to zero inflation in this case, which makes it even a little bit more challenging to model it well. But Jeff used a package here that actually helps assist with modeling zero inflation. So I hope you enjoyed the lecture. That's the end of the lecture on Poisson data to get you started. I'm hoping you can use a lot of your skills from binary logistic regression analysis and your skills from linear regression and multi-variant regression, and just apply them directly in our Poisson examples here. Next lecture is the final lecture of the series. It's kind of fun little lecture with just some kind of a bonus content of a smattering of things you can do with later models that I think are kind of cool. So look forward to seeing you next time, and thanks for taking the class."