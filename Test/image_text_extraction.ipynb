{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('Data\\Images\\IMG_20201031_182329.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('image', image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = image.shape[0]\n",
    "width = image.shape[1]\n",
    "size = (height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cv2.resize(image, (int(width/2), int(height/2)), interpolation = cv2.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('result.jpg', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, thresh1 = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU, cv2.THRESH_BINARY_INV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify structure shape and kernel size.  \n",
    "# Kernel size increases or decreases the area  \n",
    "# of the rectangle to be detected. \n",
    "# A smaller value like (10, 10) will detect  \n",
    "# each word instead of a sentence. \n",
    "rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (18, 18)) \n",
    "  \n",
    "# Appplying dilation on the threshold image \n",
    "dilation = cv2.dilate(thresh1, rect_kernel, iterations = 1) \n",
    "  \n",
    "# Finding contours \n",
    "contours, hierarchy = cv2.findContours(dilation, cv2.RETR_EXTERNAL,  \n",
    "                                                 cv2.CHAIN_APPROX_NONE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "im2 = image.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('recognized.txt', 'w+')\n",
    "file.write('')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt in contours:\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    \n",
    "    rect = cv2.rectangle(im2, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    cropped = im2[y:y + h, x:x + w]\n",
    "    file = open('recognized.txt', 'a')\n",
    "    text = pytesseract.image_to_string(cropped)\n",
    "    file.write(text)\n",
    "    file.write('\\n')\n",
    "    file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++++++++++++++++++++++++++++++++++++#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tesseract on a well defined image without any modifications on the image. Summarizing the text obtained using LexRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('Data/Images/IMG_20201031_182329_cropped.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priest’s kin say he was facing threats\n",
      "\n",
      "> Continued from Page 1\n",
      "\n",
      "oshiarpur SSP Elanche-\n",
      "\n",
      "zian said that police wo-\n",
      "\n",
      "uld exame certain arti-\n",
      "\n",
      "cles, including the priest's be-\n",
      "longings, recovered from his\n",
      "room. But Kuriakose’s family\n",
      "\n",
      "has sought a Kerala police pro-\n",
      "\n",
      "be, saying the priest was con-\n",
      "\n",
      "cerned about his safety after\n",
      "\n",
      "corroborating the nun’s claim.\n",
      "\n",
      "“There was an attempt on\n",
      "\n",
      "his life. His house and car were\n",
      "vandalised. We have no trust in\n",
      "the probe by Jalandhar police\n",
      "who are under the influence of\n",
      "the bishop,” his elder brother\n",
      "\n",
      "supporting the complainant\n",
      "nun. Another brother Jose Ku-\n",
      "rian Kattuthara alleged that\n",
      "Kuriakose was_ repeatedly\n",
      "threatened by people close to\n",
      "bishop Mulakkal. “Three days\n",
      "ago, he had told me that he was\n",
      "ing threats and hooliga-\n",
      "nism,” he added.\n",
      "\n",
      "The priest’s cook Madan\n",
      "noticed his unusual absence\n",
      "on Monday morningand infor-\n",
      "med Sister Elizabeth, the prin-\n",
      "cipal of the school. “Kuriakose\n",
      "had coffee in the evening. He\n",
      "had told me that he should not\n",
      "be disturbed. At dinner time, I\n",
      "knocked on his door, but he did\n",
      "not respond. I did not bother\n",
      "him, assuming that he was sle-\n",
      "eping,” said Madan.\n",
      "\n",
      "Sister Elizabeth said she\n",
      "called an ambulance after her\n",
      "repeated knocks went unarnis-\n",
      "wered. Father Libin Kolenche-\n",
      "ery, a priest in neighbouring\n",
      "townof Mukerian, said he was\n",
      "informed by the nuns and they\n",
      "broke open Kuriakose’s bedro-\n",
      "om door before the police arri-\n",
      "\n",
      " \n",
      "\n",
      "Alappuzha police chief S\n",
      "Surendran said a complaint\n",
      "lodged by the priest’s brothers\n",
      "had been forwarded to the Ja-\n",
      "landhar police commissioner.\n",
      "“Weare notable todoanything\n",
      "\n",
      "as the incident took place in.Ja-\n",
      "landhar,” hesaid.\n",
      "\n",
      "rest, said, “We feel Father Kuri-\n",
      "akose’s death is a warning for\n",
      "us. Don’t know what is in store\n",
      "for us, our relatives and sup\n",
      "ce appa\n",
      "\n",
      "es ge\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "text = pytesseract.image_to_string(img, lang='eng')\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('image', img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sumy\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = PlaintextParser.from_string(text, Tokenizer('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "lexRank = LexRankSummarizer()\n",
    "lexRank_summary = lexRank(text.document, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have no trust in the probe by Jalandhar police who are under the influence of the bishop,” his elder brother\n",
      "The priest’s cook Madan noticed his unusual absence on Monday morningand infor- med Sister Elizabeth, the prin- cipal of the school.\n",
      "Sister Elizabeth said she called an ambulance after her repeated knocks went unarnis- wered.\n",
      "Father Libin Kolenche- ery, a priest in neighbouring townof Mukerian, said he was informed by the nuns and they broke open Kuriakose’s bedro- om door before the police arri-\n",
      "Alappuzha police chief S Surendran said a complaint lodged by the priest’s brothers had been forwarded to the Ja- landhar police commissioner.\n"
     ]
    }
   ],
   "source": [
    "for i in lexRank_summary:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (8.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++++++++++++++++ # +++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('Data/Images/Screenshot 2020-11-05 071246.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 3. Linear Methods for Regression\n",
      "\n",
      "On the prostate cancer example, best-subset, forward and backward se\n",
      "lection all gave exactly the same sequence of terms.\n",
      "\n",
      "Some software packages implement hybrid stepwise-sclection strategies\n",
      "\n",
      "that consider both forward and backward moves at each step, and select\n",
      "the “best” of the two. For example in the R package the step function uses\n",
      "the AIC criterion for weighing the choices, which takes proper account of\n",
      "the number of parameters fit; at each step an add or drop will be performed\n",
      "‘that minimizes the AIC score.\n",
      "Other more traditional packages base the selection on F-statisties, adding\n",
      "ignificant” terms, and dropping “non-significant” terms. These are out\n",
      "of fashion, since they do not take proper account of the multiple testing\n",
      "sues. It is also tempting after a model search to print out a summary of\n",
      "‘the chosen model, such as in Table 3.2; however, the standard errors are\n",
      "not valid, since they do not account for the search process. The bootstrap\n",
      "(Section 8.2) can be useful in such settings.\n",
      "\n",
      "Finally, we note that often variables come in groups (such as the dummy\n",
      "variables that code a multi-level categorical predictor). Smart stepwise pro-\n",
      "cedures (stich as step in R) will add or drop whole groups at a time, taking\n",
      "proper account of their degrees-of-freedom.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "3.3.3 Forwarl-Stagewise Regression\n",
      "\n",
      "Forward-stagewise regression (FS) is even more constrained than forward-\n",
      "stepwise regression. It starts like forward-stepwise regression, with am in-\n",
      "tercept equal to J, and centered predictors with coefficients initially all 0.\n",
      "Atv each step the algorithm identifies the variable most correlated with the\n",
      "current residual. It then computes the simple linear regression coefficient\n",
      "of the residual on this chosen variable, and then adds it to the current co-\n",
      "efficient for that variable. This is continued till none of the variables have\n",
      "correlation with the residuals—ie. the least-squares fit when N > p.\n",
      "\n",
      "Unlike forward-stepwise regression, none of the other variables are ad-\n",
      "Justed when a term is added to the model. As a consequence, forward\n",
      "Sstagewise can take many more than p steps to reach the least squares fit,\n",
      "\n",
      "istorically has been dismissed as being inefficient. It turns out that\n",
      "high-dimensional problems. We\n",
      "see in Section 3.8.1 that both forward stagewise and a variant which is\n",
      "slowed down even further are quite competitive, especially in very high-\n",
      "dimensional problems.\n",
      "\n",
      "Forward-stagewise regression is included in Figure 3.6. In this example it\n",
      "takes over 1000 steps to get all the correlations below 10~, For subset size\n",
      "k, we plotted the error for the last step for which there where & nonzero\n",
      "coefficients. Although it catches up with the best fit, it takes longer to\n",
      "dos\n",
      "\n",
      "    \n",
      "\n",
      " \n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "text = pytesseract.image_to_string(img, lang='eng')\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = cv2.imread('Data/Images/Screenshot 2020-11-05 071339.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344 Shrinkage Methods 61\n",
      "\n",
      "3.3.4 Prostate Cancer Data Example (Continued)\n",
      "\n",
      "Table 3.3 shows the coefficients from a number of different selection and\n",
      "shrinkage methods. They are best-subset selection using an all-subsets search,\n",
      "ridge regression, the lasso, principal components regression and partial least\n",
      "squares. Each method has a complexity parameter, and this was chasen to\n",
      "minimize an estimate of prediction error based on tenfold cross-validation;\n",
      "full details are given in Section 7.10. Briefly, cross-validation works by divid-\n",
      "ing the training data randomly into ten equal parts. The leaning method\n",
      "is fit—for a range of values of the complexity parameter—to nine-tenths of\n",
      "‘the data, and the prediction error is computed on the remaining one-tenth,\n",
      "‘This is done in turn for each one-tenth of the data, and the ten prediction\n",
      "error estimates are averaged. From this we obtain an estimated prediction\n",
      "error curve as a function of the complexity parameter.\n",
      "\n",
      "Note that we have already divided these data into a training set of size\n",
      "67 and a test set of size 30. Cross-validation is applied to the training set,\n",
      "since selecting the shrinkage parameter is part of the training process. The\n",
      "test set is there to judge the performance of the selected model.\n",
      "\n",
      "‘The estimated prediction error curves are shown in Figure 3.7. Many of\n",
      "the curves are very flat over large ranges near their minimum, Included\n",
      "are estimated standard error bands for each estimated error rate, based on\n",
      "the ten error estimates computed by cross-validation. We have used the\n",
      "“one-standard-error” rule—we pick the most parsimonious model within\n",
      "fone standard error of the minimum (Section 7.10, page 244). Such a rule\n",
      "acknowledges the fact that the tradeoff curve is estimated with error, and\n",
      "hence takes a conservative approach.\n",
      "\n",
      "Best-subset selection chose to use the two predictors 1evol and 1weight.\n",
      "‘The last two lines of the table give the average prediction error (and its\n",
      "estimated standard error) over the test set.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "3.4 Shrinkage Methods\n",
      "\n",
      "By retaining a subset of the predictors and discarding the rest, subset selee-\n",
      "tion produces a model that is interpretable and has passibly lower predic-\n",
      "tion error than the full model. However, because it is a diserete process—\n",
      "variables are either retained or discarded—it often exhibits high variance,\n",
      "and so doesn’t reduce the prediction error of the full model. Shrinkage\n",
      "methods are more continuous, and don’t suffer as much from high\n",
      "variability.\n",
      "\n",
      "3.4.1 Ridge Regression\n",
      "\n",
      "Ridge regression shrinks the regression coefficients by imposing a penalty\n",
      "on their size. The ridge coefficients minimize a penalized residual sum of\n",
      "\n",
      " \n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "text2 = pytesseract.image_to_string(img2, lang='eng')\n",
    "\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 3. Linear Methods for Regression\n",
      "\n",
      "On the prostate cancer example, best-subset, forward and backward se\n",
      "lection all gave exactly the same sequence of terms.\n",
      "\n",
      "Some software packages implement hybrid stepwise-sclection strategies\n",
      "\n",
      "that consider both forward and backward moves at each step, and select\n",
      "the “best” of the two. For example in the R package the step function uses\n",
      "the AIC criterion for weighing the choices, which takes proper account of\n",
      "the number of parameters fit; at each step an add or drop will be performed\n",
      "‘that minimizes the AIC score.\n",
      "Other more traditional packages base the selection on F-statisties, adding\n",
      "ignificant” terms, and dropping “non-significant” terms. These are out\n",
      "of fashion, since they do not take proper account of the multiple testing\n",
      "sues. It is also tempting after a model search to print out a summary of\n",
      "‘the chosen model, such as in Table 3.2; however, the standard errors are\n",
      "not valid, since they do not account for the search process. The bootstrap\n",
      "(Section 8.2) can be useful in such settings.\n",
      "\n",
      "Finally, we note that often variables come in groups (such as the dummy\n",
      "variables that code a multi-level categorical predictor). Smart stepwise pro-\n",
      "cedures (stich as step in R) will add or drop whole groups at a time, taking\n",
      "proper account of their degrees-of-freedom.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "3.3.3 Forwarl-Stagewise Regression\n",
      "\n",
      "Forward-stagewise regression (FS) is even more constrained than forward-\n",
      "stepwise regression. It starts like forward-stepwise regression, with am in-\n",
      "tercept equal to J, and centered predictors with coefficients initially all 0.\n",
      "Atv each step the algorithm identifies the variable most correlated with the\n",
      "current residual. It then computes the simple linear regression coefficient\n",
      "of the residual on this chosen variable, and then adds it to the current co-\n",
      "efficient for that variable. This is continued till none of the variables have\n",
      "correlation with the residuals—ie. the least-squares fit when N > p.\n",
      "\n",
      "Unlike forward-stepwise regression, none of the other variables are ad-\n",
      "Justed when a term is added to the model. As a consequence, forward\n",
      "Sstagewise can take many more than p steps to reach the least squares fit,\n",
      "\n",
      "istorically has been dismissed as being inefficient. It turns out that\n",
      "high-dimensional problems. We\n",
      "see in Section 3.8.1 that both forward stagewise and a variant which is\n",
      "slowed down even further are quite competitive, especially in very high-\n",
      "dimensional problems.\n",
      "\n",
      "Forward-stagewise regression is included in Figure 3.6. In this example it\n",
      "takes over 1000 steps to get all the correlations below 10~, For subset size\n",
      "k, we plotted the error for the last step for which there where & nonzero\n",
      "coefficients. Although it catches up with the best fit, it takes longer to\n",
      "dos\n",
      "\n",
      "    \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "344 Shrinkage Methods 61\n",
      "\n",
      "3.3.4 Prostate Cancer Data Example (Continued)\n",
      "\n",
      "Table 3.3 shows the coefficients from a number of different selection and\n",
      "shrinkage methods. They are best-subset selection using an all-subsets search,\n",
      "ridge regression, the lasso, principal components regression and partial least\n",
      "squares. Each method has a complexity parameter, and this was chasen to\n",
      "minimize an estimate of prediction error based on tenfold cross-validation;\n",
      "full details are given in Section 7.10. Briefly, cross-validation works by divid-\n",
      "ing the training data randomly into ten equal parts. The leaning method\n",
      "is fit—for a range of values of the complexity parameter—to nine-tenths of\n",
      "‘the data, and the prediction error is computed on the remaining one-tenth,\n",
      "‘This is done in turn for each one-tenth of the data, and the ten prediction\n",
      "error estimates are averaged. From this we obtain an estimated prediction\n",
      "error curve as a function of the complexity parameter.\n",
      "\n",
      "Note that we have already divided these data into a training set of size\n",
      "67 and a test set of size 30. Cross-validation is applied to the training set,\n",
      "since selecting the shrinkage parameter is part of the training process. The\n",
      "test set is there to judge the performance of the selected model.\n",
      "\n",
      "‘The estimated prediction error curves are shown in Figure 3.7. Many of\n",
      "the curves are very flat over large ranges near their minimum, Included\n",
      "are estimated standard error bands for each estimated error rate, based on\n",
      "the ten error estimates computed by cross-validation. We have used the\n",
      "“one-standard-error” rule—we pick the most parsimonious model within\n",
      "fone standard error of the minimum (Section 7.10, page 244). Such a rule\n",
      "acknowledges the fact that the tradeoff curve is estimated with error, and\n",
      "hence takes a conservative approach.\n",
      "\n",
      "Best-subset selection chose to use the two predictors 1evol and 1weight.\n",
      "‘The last two lines of the table give the average prediction error (and its\n",
      "estimated standard error) over the test set.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "3.4 Shrinkage Methods\n",
      "\n",
      "By retaining a subset of the predictors and discarding the rest, subset selee-\n",
      "tion produces a model that is interpretable and has passibly lower predic-\n",
      "tion error than the full model. However, because it is a diserete process—\n",
      "variables are either retained or discarded—it often exhibits high variance,\n",
      "and so doesn’t reduce the prediction error of the full model. Shrinkage\n",
      "methods are more continuous, and don’t suffer as much from high\n",
      "variability.\n",
      "\n",
      "3.4.1 Ridge Regression\n",
      "\n",
      "Ridge regression shrinks the regression coefficients by imposing a penalty\n",
      "on their size. The ridge coefficients minimize a penalized residual sum of\n",
      "\n",
      " \n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_text = text+'\\n'+text2\n",
    "\n",
    "print(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['60 3. Linear Methods for Regression\\n\\nOn the prostate cancer example, best-subset, forward and backward se\\nlection all gave exactly the same sequence of terms.\\n\\nSome software packages implement hybrid stepwise-sclection strategies\\n\\nthat consider both forward and backward moves at each step, and select\\nthe “best” of the two. For example in the R package the step function uses\\nthe AIC criterion for weighing the choices, which takes proper account of\\nthe number of parameters fit; at each step an add or drop will be performed\\n‘that minimizes the AIC score.\\nOther more traditional packages base the selection on F-statisties, adding\\nignificant” terms, and dropping “non-significant” terms. These are out\\nof fashion, since they do not take proper account of the multiple testing\\nsues. It is also tempting after a model search to print out a summary of\\n‘the chosen model, such as in Table 3.2; however, the standard errors are\\nnot valid, since they do not account for the search process. The bootstrap\\n(Section 8.2) can be useful in such settings.\\n\\nFinally, we note that often variables come in groups (such as the dummy\\nvariables that code a multi-level categorical predictor). Smart stepwise pro-\\ncedures (stich as step in R) will add or drop whole groups at a time, taking\\nproper account of their degrees-of-freedom.\\n\\n \\n\\n \\n\\n \\n\\n3.3.3 Forwarl-Stagewise Regression\\n\\nForward-stagewise regression (FS) is even more constrained than forward-\\nstepwise regression. It starts like forward-stepwise regression, with am in-\\ntercept equal to J, and centered predictors with coefficients initially all 0.\\nAtv each step the algorithm identifies the variable most correlated with the\\ncurrent residual. It then computes the simple linear regression coefficient\\nof the residual on this chosen variable, and then adds it to the current co-\\nefficient for that variable. This is continued till none of the variables have\\ncorrelation with the residuals—ie. the least-squares fit when N > p.\\n\\nUnlike forward-stepwise regression, none of the other variables are ad-\\nJusted when a term is added to the model. As a consequence, forward\\nSstagewise can take many more than p steps to reach the least squares fit,\\n\\nistorically has been dismissed as being inefficient. It turns out that\\nhigh-dimensional problems. We\\nsee in Section 3.8.1 that both forward stagewise and a variant which is\\nslowed down even further are quite competitive, especially in very high-\\ndimensional problems.\\n\\nForward-stagewise regression is included in Figure 3.6. In this example it\\ntakes over 1000 steps to get all the correlations below 10~, For subset size\\nk, we plotted the error for the last step for which there where & nonzero\\ncoefficients. Although it catches up with the best fit, it takes longer to\\ndos\\n\\n    \\n\\n \\n\\x0c\\n344 Shrinkage Methods 61\\n\\n3.3.4 Prostate Cancer Data Example (Continued)\\n\\nTable 3.3 shows the coefficients from a number of different selection and\\nshrinkage methods. They are best-subset selection using an all-subsets search,\\nridge regression, the lasso, principal components regression and partial least\\nsquares. Each method has a complexity parameter, and this was chasen to\\nminimize an estimate of prediction error based on tenfold cross-validation;\\nfull details are given in Section 7.10. Briefly, cross-validation works by divid-\\ning the training data randomly into ten equal parts. The leaning method\\nis fit—for a range of values of the complexity parameter—to nine-tenths of\\n‘the data, and the prediction error is computed on the remaining one-tenth,\\n‘This is done in turn for each one-tenth of the data, and the ten prediction\\nerror estimates are averaged. From this we obtain an estimated prediction\\nerror curve as a function of the complexity parameter.\\n\\nNote that we have already divided these data into a training set of size\\n67 and a test set of size 30. Cross-validation is applied to the training set,\\nsince selecting the shrinkage parameter is part of the training process. The\\ntest set is there to judge the performance of the selected model.\\n\\n‘The estimated prediction error curves are shown in Figure 3.7. Many of\\nthe curves are very flat over large ranges near their minimum, Included\\nare estimated standard error bands for each estimated error rate, based on\\nthe ten error estimates computed by cross-validation. We have used the\\n“one-standard-error” rule—we pick the most parsimonious model within\\nfone standard error of the minimum (Section 7.10, page 244). Such a rule\\nacknowledges the fact that the tradeoff curve is estimated with error, and\\nhence takes a conservative approach.\\n\\nBest-subset selection chose to use the two predictors 1evol and 1weight.\\n‘The last two lines of the table give the average prediction error (and its\\nestimated standard error) over the test set.\\n\\n \\n\\n \\n\\n3.4 Shrinkage Methods\\n\\nBy retaining a subset of the predictors and discarding the rest, subset selee-\\ntion produces a model that is interpretable and has passibly lower predic-\\ntion error than the full model. However, because it is a diserete process—\\nvariables are either retained or discarded—it often exhibits high variance,\\nand so doesn’t reduce the prediction error of the full model. Shrinkage\\nmethods are more continuous, and don’t suffer as much from high\\nvariability.\\n\\n3.4.1 Ridge Regression\\n\\nRidge regression shrinks the regression coefficients by imposing a penalty\\non their size. The ridge coefficients minimize a penalized residual sum of\\n\\n \\n\\x0c']\n"
     ]
    }
   ],
   "source": [
    "print((combined_text.split('\\n\\n[0-9]*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sumy\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = PlaintextParser.from_string(text, Tokenizer('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "lexRank = LexRankSummarizer()\n",
    "lexRank_summary = lexRank(text.document, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have no trust in the probe by Jalandhar police who are under the influence of the bishop,” his elder brother\n",
      "The priest’s cook Madan noticed his unusual absence on Monday morningand infor- med Sister Elizabeth, the prin- cipal of the school.\n",
      "Sister Elizabeth said she called an ambulance after her repeated knocks went unarnis- wered.\n",
      "Father Libin Kolenche- ery, a priest in neighbouring townof Mukerian, said he was informed by the nuns and they broke open Kuriakose’s bedro- om door before the police arri-\n",
      "Alappuzha police chief S Surendran said a complaint lodged by the priest’s brothers had been forwarded to the Ja- landhar police commissioner.\n"
     ]
    }
   ],
   "source": [
    "for i in lexRank_summary:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
